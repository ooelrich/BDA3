---
title: "Exercise 2.13"
author: "Oscar Oelrich"
date: "17 december 2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


| Year  | Fatal accidents  | Passenger deaths   | Death rate  |
|-------|------------------|--------------------|-------------|
| 1976  | 24               |  734               | 0.19        |
| 1977  | 25               |  516               | 0.12        |
| 1978  | 31               |  754               | 0.15        |
| 1979  | 31               |  877               | 0.16        |
| 1980  | 22               |  814               | 0.14        |
| 1981  | 21               |  362               | 0.06        |
| 1982  | 26               |  764               | 0.13        |
| 1983  | 20               |  809               | 0.13        |
| 1984  | 16               |  223               | 0.03        |
| 1985  | 22               |  1066              | 0.15        |


### Part a

We assume that the number of fatal accidents in a year follows a Poisson distribution with parameter $\theta$, with probability mass function given by

$$\begin{equation}
p(y|\theta) = \frac{\theta^y e^{-\theta}}{y!}.
\end{equation}$$

The conjugate prior of the Poisson is the gamma distribution, which is a distribution with two parameters ($\alpha$ and $\beta$), and density

$$\begin{equation}
p(\theta)=\frac{\beta^{\alpha}}{\Gamma(\alpha)} \theta^{\alpha-1}e^{-\beta \theta}.
\end{equation}$$

For ease of notation, we use $\boldsymbol{y}$ to refer to the observed data, and $y_{1986}$ to refer to the (future) observation we wish to predict. The likelihood for the ten observations is given by

$$\begin{equation}
L(\boldsymbol{y}|\theta)=\prod_{i=1}^{10}\frac{\theta^{y_i} e^{-\theta}}{y_i!} \propto \theta^{\sum_{i=1}^{10} y_i}e^{-n\theta}.
\end{equation}$$

Since we have a conjugate prior, we know that the posterior will be from the same family (gamma), and since the gamma distribution is fully identified by looking at the exponential term and the exponent of $\theta$ we can find it easily by multiplying the relevant parts of the prior and likelihood

$$\begin{equation}
p(\theta|\boldsymbol{y}) \propto \theta^{\alpha-1}e^{-\beta \theta}\theta^{\sum_{i=1}^{10} y_i}e^{-n\theta}=\theta^{(\alpha + \sum_{i=1}^{10} y_i) - 1}e^{-(n+\beta)\theta},
\end{equation}$$

which tells us that the posterior must be a $gamma(\alpha + \sum_{i=1}^{10} y_i, n+\beta)$ distribution. Since we really don't have any prior information we pick $\alpha$ and $\beta$ to be arbitrarily small, and so our posterior is $gamma(\sum_{i=1}^{10} y_i, n)=gamma(238, 10)$.

Next we wish to use our posterior distribution to make a $95 \%$ predictive interval for the number of accidents the next year. The posterior predictive distribution for the number of accidents next year is given by 

$$\begin{equation}
p(y_{1986}|\boldsymbol{y}) = \int_{\theta} p(y_{1986}|\boldsymbol{y}, \theta) p(\theta|\boldsymbol{y}) d\theta \\ \qquad \quad =\int_{\theta} p(y_{1986}|\theta) p(\theta|\boldsymbol{y}) d\theta. 
\end{equation}$$

We will calculate/approximate it using two different methods: by simulation and by normal approximation.

The normal approximation approach uses the fact that if both the posterior and the data (conditional on the posterior) follow normal distributions then $p(y_{1986}|\theta) p(\theta)=p(y_{1986},\theta)$ is jointly normal and so the marginal distribution of the predicted value is also normal. All we need to do is to find the mean and variance of the distribution.

Using the law of total expectation, we find the mean to be

$$\begin{equation}
\mathbb{E}(y_{1986}|\boldsymbol{y})=\mathbb{E}_{\theta|\boldsymbol{y}}(\mathbb{E}(y_{1986}|\boldsymbol{y},(\theta|\boldsymbol{y}))=\mathbb{E}_{\theta|\boldsymbol{y}}(\mathbb{E}(y_{1986}|(\theta|\boldsymbol{y})),
\end{equation}$$

where the last step follows from the fact that the old observations only influence the expected number of accidents in the future through the posterior distribution. Further

$$\begin{equation}
\mathbb{E}_{\theta|\boldsymbol{y}}(\mathbb{E}(y_{1986}|(\theta|\boldsymbol{y}))=\mathbb{E}_{\theta|\boldsymbol{y}}(\theta|\boldsymbol{y})=\frac{238}{10}=23.8,
\end{equation}$$

where we use the fact that the expectation of the predictive distribution is $\theta$ (using the normal approximation of a $Poisson(\theta)$ distribution, we get that the data follows a $N(\theta, \theta)$ distribution, and thus has mean/expectation equal to $\theta$), and that the mean of a $gamma(\alpha, \beta)$ is $\frac{\alpha}{\beta}$.

In order to find the variance we need to use the formula for conditional variance

$$\begin{align}
\mathrm{var}(y_{1986}|\boldsymbol{y})&=\mathbb{E}_{\theta|\boldsymbol{y}}(\mathrm{var}(y_{1986}|\boldsymbol{y},(\theta|\boldsymbol{y})))+\mathrm{var}_{\theta|\boldsymbol{y}}(\mathbb{E}(y_{1986}|\boldsymbol{y},(\theta|\boldsymbol{y}))) \\
&= \mathbb{E}_{\theta|\boldsymbol{y}}(\mathrm{var}(y_{1986}|\boldsymbol{y}))+\mathrm{var}_{\theta|\boldsymbol{y}}(\mathbb{E}(y_{1986}|(\theta|\boldsymbol{y})))   \\
&=\mathbb{E}_{\theta|\boldsymbol{y}}(\theta|\boldsymbol{y})+\mathrm{var}_{\theta|\boldsymbol{y}}(\theta|\boldsymbol{y}) \\
&= 23.8 + 2.38 = 5.12^2, 
\end{align}$$

where again we use that the variance of the data is $\theta$ and that the variance of the gamma distribution is $\frac{\alpha}{\beta^2}$. So the posterior predictive distribution, using normal approximation, is $N(23.8, 5.12^2)$. From this we can easily construct a $95 \%$ predictive interval as $[23.8 \pm 5.12*1.96]=[13.8, 33.8]$ which, considering that the number of accidents is a discrete number, gives the interval $[13, 34]$.

The other way of solving the exercise is to use Monte Carlo simulation: sample from the posterior distribution and then for each simulated value generate an observation from a Poisson distribution with intensity equal to the simulated value. We can then use the required percentiles of this simulated data to find the $95 \%$ predictive interval.


```{r}
#  generate draws from the posterior
n_obs <- 10000 # number of simulations
theta <- rgamma(n_obs, 238, 10)
y_gen <- rpois(n_obs, theta)
print(sort(y_gen)[c(0.025, 0.975)*n_obs]) # computed interval is (13, 34)
```

Quite close to our normal approximation.

### Part b




### Part c




### Part d




### Part e